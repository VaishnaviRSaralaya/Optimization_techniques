{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Optimization Algorithms from scratch implementations and library implementations."
      ],
      "metadata": {
        "id": "HqEiFJ_v5O99"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Stochastic Gradient Descent (SGD) from scatch"
      ],
      "metadata": {
        "id": "okUcxusH5Oj1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iby8HJyZ5L8u",
        "outputId": "bfe49c4d-54eb-4639-b313-473d320e0354"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "Batch 0/1875, Loss: 2.414673089981079\n",
            "Batch 100/1875, Loss: 1.5826143026351929\n",
            "Batch 200/1875, Loss: 0.9501537680625916\n",
            "Batch 300/1875, Loss: 0.825154185295105\n",
            "Batch 400/1875, Loss: 0.765343189239502\n",
            "Batch 500/1875, Loss: 0.8656417727470398\n",
            "Batch 600/1875, Loss: 0.44429463148117065\n",
            "Batch 700/1875, Loss: 0.4542093873023987\n",
            "Batch 800/1875, Loss: 0.41611939668655396\n",
            "Batch 900/1875, Loss: 0.3259173035621643\n",
            "Batch 1000/1875, Loss: 0.5184282064437866\n",
            "Batch 1100/1875, Loss: 0.4106502830982208\n",
            "Batch 1200/1875, Loss: 0.4260193109512329\n",
            "Batch 1300/1875, Loss: 0.5035765171051025\n",
            "Batch 1400/1875, Loss: 0.38824009895324707\n",
            "Batch 1500/1875, Loss: 0.298125684261322\n",
            "Batch 1600/1875, Loss: 0.33617550134658813\n",
            "Batch 1700/1875, Loss: 0.2514995336532593\n",
            "Batch 1800/1875, Loss: 0.34522655606269836\n",
            "Epoch 2/5\n",
            "Batch 0/1875, Loss: 0.42509835958480835\n",
            "Batch 100/1875, Loss: 0.4461454749107361\n",
            "Batch 200/1875, Loss: 0.3921183943748474\n",
            "Batch 300/1875, Loss: 0.26688694953918457\n",
            "Batch 400/1875, Loss: 0.28448840975761414\n",
            "Batch 500/1875, Loss: 0.4674832820892334\n",
            "Batch 600/1875, Loss: 0.2570335268974304\n",
            "Batch 700/1875, Loss: 0.20826095342636108\n",
            "Batch 800/1875, Loss: 0.2511236369609833\n",
            "Batch 900/1875, Loss: 0.1630837619304657\n",
            "Batch 1000/1875, Loss: 0.4283406734466553\n",
            "Batch 1100/1875, Loss: 0.26977676153182983\n",
            "Batch 1200/1875, Loss: 0.308594286441803\n",
            "Batch 1300/1875, Loss: 0.36230796575546265\n",
            "Batch 1400/1875, Loss: 0.3135213255882263\n",
            "Batch 1500/1875, Loss: 0.2159561961889267\n",
            "Batch 1600/1875, Loss: 0.26055005192756653\n",
            "Batch 1700/1875, Loss: 0.17432743310928345\n",
            "Batch 1800/1875, Loss: 0.30027690529823303\n",
            "Epoch 3/5\n",
            "Batch 0/1875, Loss: 0.32182079553604126\n",
            "Batch 100/1875, Loss: 0.40125876665115356\n",
            "Batch 200/1875, Loss: 0.34674784541130066\n",
            "Batch 300/1875, Loss: 0.2053402066230774\n",
            "Batch 400/1875, Loss: 0.22239801287651062\n",
            "Batch 500/1875, Loss: 0.3949388265609741\n",
            "Batch 600/1875, Loss: 0.21491366624832153\n",
            "Batch 700/1875, Loss: 0.15947109460830688\n",
            "Batch 800/1875, Loss: 0.19762162864208221\n",
            "Batch 900/1875, Loss: 0.1206851601600647\n",
            "Batch 1000/1875, Loss: 0.4065316915512085\n",
            "Batch 1100/1875, Loss: 0.23265981674194336\n",
            "Batch 1200/1875, Loss: 0.26077741384506226\n",
            "Batch 1300/1875, Loss: 0.29245686531066895\n",
            "Batch 1400/1875, Loss: 0.2834746241569519\n",
            "Batch 1500/1875, Loss: 0.19105370342731476\n",
            "Batch 1600/1875, Loss: 0.22116997838020325\n",
            "Batch 1700/1875, Loss: 0.14473402500152588\n",
            "Batch 1800/1875, Loss: 0.2731756567955017\n",
            "Epoch 4/5\n",
            "Batch 0/1875, Loss: 0.26291388273239136\n",
            "Batch 100/1875, Loss: 0.38141319155693054\n",
            "Batch 200/1875, Loss: 0.31488409638404846\n",
            "Batch 300/1875, Loss: 0.17263373732566833\n",
            "Batch 400/1875, Loss: 0.19401398301124573\n",
            "Batch 500/1875, Loss: 0.35518011450767517\n",
            "Batch 600/1875, Loss: 0.1896297186613083\n",
            "Batch 700/1875, Loss: 0.13191062211990356\n",
            "Batch 800/1875, Loss: 0.1670045554637909\n",
            "Batch 900/1875, Loss: 0.10345098376274109\n",
            "Batch 1000/1875, Loss: 0.39542102813720703\n",
            "Batch 1100/1875, Loss: 0.21551260352134705\n",
            "Batch 1200/1875, Loss: 0.23271164298057556\n",
            "Batch 1300/1875, Loss: 0.24598687887191772\n",
            "Batch 1400/1875, Loss: 0.2619941234588623\n",
            "Batch 1500/1875, Loss: 0.17173534631729126\n",
            "Batch 1600/1875, Loss: 0.1908949911594391\n",
            "Batch 1700/1875, Loss: 0.12345029413700104\n",
            "Batch 1800/1875, Loss: 0.2482846975326538\n",
            "Epoch 5/5\n",
            "Batch 0/1875, Loss: 0.21857404708862305\n",
            "Batch 100/1875, Loss: 0.3653940260410309\n",
            "Batch 200/1875, Loss: 0.29010772705078125\n",
            "Batch 300/1875, Loss: 0.14834365248680115\n",
            "Batch 400/1875, Loss: 0.17624834179878235\n",
            "Batch 500/1875, Loss: 0.3257165551185608\n",
            "Batch 600/1875, Loss: 0.17187324166297913\n",
            "Batch 700/1875, Loss: 0.1119113340973854\n",
            "Batch 800/1875, Loss: 0.14717474579811096\n",
            "Batch 900/1875, Loss: 0.09286726266145706\n",
            "Batch 1000/1875, Loss: 0.38398444652557373\n",
            "Batch 1100/1875, Loss: 0.20335298776626587\n",
            "Batch 1200/1875, Loss: 0.21004198491573334\n",
            "Batch 1300/1875, Loss: 0.21560630202293396\n",
            "Batch 1400/1875, Loss: 0.24395610392093658\n",
            "Batch 1500/1875, Loss: 0.15711669623851776\n",
            "Batch 1600/1875, Loss: 0.1746978759765625\n",
            "Batch 1700/1875, Loss: 0.10857593268156052\n",
            "Batch 1800/1875, Loss: 0.2313416749238968\n",
            "Test Accuracy: 0.9362999796867371\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "\n",
        "# Load and preprocess MNIST dataset of handwritten digits\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize pixel values\n",
        "x_train = x_train.reshape(-1, 28 * 28)  # Flatten 28x28 images to 784-dim vectors\n",
        "x_test = x_test.reshape(-1, 28 * 28)\n",
        "y_train = to_categorical(y_train, 10)  # Convert labels to one-hot encoding\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Build neural network model\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(28 * 28,)), #relu introduces non-linearity\n",
        "    Dense(10, activation='softmax')])# Output layer with probability distribution\n",
        "\n",
        "# Define a custom Stochastic Gradient Descent (SGD) optimizer from scratch\n",
        "class CustomSGD:\n",
        "    def __init__(self, learning_rate=0.01):\n",
        "        # Initialize learning rate for weight updates\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def update(self, weights, grads):\n",
        "        \"\"\"\n",
        "        Update weights using gradient descent.\n",
        "        :param weights: List of model weights (trainable variables)\n",
        "        :param grads: List of gradients corresponding to the weights\n",
        "        \"\"\"\n",
        "        for i in range(len(weights)):\n",
        "            # Update each weight: weight = weight - learning_rate * gradient\n",
        "            weights[i].assign_sub(self.learning_rate * grads[i])\n",
        "\n",
        "# Define categorical cross-entropy loss function for multi-class classification\n",
        "def categorical_crossentropy(y_true, y_pred):\n",
        "    # Calculate negative log likelihood of true labels given predictions\n",
        "    return -tf.reduce_mean(tf.reduce_sum(y_true * tf.math.log(y_pred), axis=1))\n",
        "\n",
        "# Training loop function\n",
        "def train_model(model, x_train, y_train, epochs=5, batch_size=32, learning_rate=0.01):\n",
        "    # Initialize our custom SGD optimizer with given learning rate\n",
        "    optimizer = CustomSGD(learning_rate=learning_rate)\n",
        "\n",
        "    # Calculate number of batches per epoch\n",
        "    num_samples = x_train.shape[0]\n",
        "    num_batches = int(np.ceil(num_samples / batch_size))\n",
        "\n",
        "    # Training loop over epochs\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "\n",
        "        # Process each batch in the epoch\n",
        "        for batch in range(num_batches):\n",
        "            # Get current batch of data\n",
        "            start = batch * batch_size\n",
        "            end = start + batch_size\n",
        "            x_batch = x_train[start:end]\n",
        "            y_batch = y_train[start:end]\n",
        "\n",
        "            # Forward pass: compute predictions and loss\n",
        "            with tf.GradientTape() as tape:\n",
        "                y_pred = model(x_batch, training=True)\n",
        "                loss = categorical_crossentropy(y_batch, y_pred)\n",
        "\n",
        "            # Backward pass: compute gradients of loss with respect to weights\n",
        "            grads = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "            # Update weights using our custom optimizer\n",
        "            optimizer.update(model.trainable_variables, grads)\n",
        "\n",
        "            # Print progress every 100 batches\n",
        "            if batch % 100 == 0:\n",
        "                print(f\"Batch {batch}/{num_batches}, Loss: {loss.numpy()}\")\n",
        "\n",
        "# Train the model for 5 epochs with batch size 32 and learning rate 0.01\n",
        "train_model(model, x_train, y_train, epochs=5, batch_size=32, learning_rate=0.01)\n",
        "\n",
        "# Function to evaluate model performance on test set\n",
        "def evaluate_model(model, x_test, y_test):\n",
        "    # Get model predictions\n",
        "    y_pred = model(x_test)\n",
        "\n",
        "    # Convert predictions and true labels from one-hot to class indices\n",
        "    y_pred = tf.argmax(y_pred, axis=1)\n",
        "    y_true = tf.argmax(y_test, axis=1)\n",
        "\n",
        "    # Calculate accuracy by comparing predictions to true labels\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(y_pred, y_true), tf.float32))\n",
        "    print(f\"Test Accuracy: {accuracy.numpy()}\")\n",
        "\n",
        "# Evaluate the trained model on test data\n",
        "evaluate_model(model, x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. SGD with momentum from scatch"
      ],
      "metadata": {
        "id": "wZp8b5Lj52T8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "\n",
        "# Load and preprocess MNIST dataset of handwritten digits\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize pixel values\n",
        "x_train = x_train.reshape(-1, 28 * 28)  # Flatten 28x28 images to 784-dim vectors\n",
        "x_test = x_test.reshape(-1, 28 * 28)\n",
        "y_train = to_categorical(y_train, 10)  # Convert labels to one-hot encoding\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Build neural network model\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(28 * 28,)), #relu introduces non-linearity\n",
        "    Dense(10, activation='softmax') ]) # Output layer with probability distribution\n",
        "\n",
        "# Define a custom SGD with Momentum optimizer\n",
        "class SGDMomentum:\n",
        "    def __init__(self, learning_rate=0.01, momentum=0.9):\n",
        "        self.learning_rate = learning_rate  # Step size for weight updates\n",
        "        self.momentum = momentum  # Controls how much of past gradients are retained (0.9 is common)\n",
        "        self.velocities = None  # Will store velocity terms for each weight (initialized in first update)\n",
        "\n",
        "    def update(self, weights, grads):\n",
        "        \"\"\"\n",
        "        Update weights using SGD with momentum.\n",
        "        :param weights: List of model weights (trainable variables).\n",
        "        :param grads: List of gradients corresponding to the weights.\n",
        "        \"\"\"\n",
        "        if self.velocities is None:\n",
        "            # Initialize velocities as zero vectors matching the shape of each weight tensor\n",
        "            self.velocities = [tf.Variable(tf.zeros_like(w)) for w in weights]\n",
        "\n",
        "        for i in range(len(weights)):\n",
        "            # Update velocity: v = momentum * v - learning_rate * grad (momentum accumulates past gradients)\n",
        "            self.velocities[i].assign(self.momentum * self.velocities[i] - self.learning_rate * grads[i])\n",
        "            # Update weights: w = w + v (applies the smoothed velocity instead of raw gradients)\n",
        "            weights[i].assign_add(self.velocities[i])\n",
        "\n",
        "# Loss function (categorical cross-entropy)\n",
        "def categorical_crossentropy(y_true, y_pred):\n",
        "    # Computes cross-entropy loss between true labels (one-hot) and predicted probabilities\n",
        "    return -tf.reduce_mean(tf.reduce_sum(y_true * tf.math.log(y_pred), axis=1))\n",
        "\n",
        "# Training loop\n",
        "def train_model(model, x_train, y_train, epochs=5, batch_size=32, learning_rate=0.01, momentum=0.9):\n",
        "    optimizer = SGDMomentum(learning_rate=learning_rate, momentum=momentum)\n",
        "    num_samples = x_train.shape[0]  # Total training samples (60,000)\n",
        "    num_batches = int(np.ceil(num_samples / batch_size))  # Number of batches per epoch\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "        for batch in range(num_batches):\n",
        "            # Get a batch of data\n",
        "            start = batch * batch_size\n",
        "            end = start + batch_size\n",
        "            x_batch = x_train[start:end]  # Input batch (shape: [batch_size, 784])\n",
        "            y_batch = y_train[start:end]  # Label batch (shape: [batch_size, 10])\n",
        "\n",
        "            # Forward pass (compute predictions under gradient tape for autodiff)\n",
        "            with tf.GradientTape() as tape:\n",
        "                y_pred = model(x_batch, training=True)  # Model outputs (probabilities)\n",
        "                loss = categorical_crossentropy(y_batch, y_pred)  # Compute loss\n",
        "\n",
        "            # Backward pass (compute gradients of loss w.r.t. trainable weights)\n",
        "            grads = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "            # Update weights using the custom SGD with Momentum optimizer\n",
        "            optimizer.update(model.trainable_variables, grads)\n",
        "\n",
        "            # Print loss every 100 batches\n",
        "            if batch % 100 == 0:\n",
        "                print(f\"Batch {batch}/{num_batches}, Loss: {loss.numpy()}\")\n",
        "\n",
        "# Train the model\n",
        "train_model(model, x_train, y_train, epochs=5, batch_size=32, learning_rate=0.01, momentum=0.9)\n",
        "\n",
        "# Evaluate the model\n",
        "def evaluate_model(model, x_test, y_test):\n",
        "    y_pred = model(x_test)  # Get model predictions (probabilities)\n",
        "    y_pred = tf.argmax(y_pred, axis=1)  # Convert to class labels (0-9)\n",
        "    y_true = tf.argmax(y_test, axis=1)  # Convert one-hot labels to class labels\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(y_pred, y_true), tf.float32))  # Compute accuracy\n",
        "    print(f\"Test Accuracy: {accuracy.numpy()}\")\n",
        "\n",
        "evaluate_model(model, x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byjLReDL5nva",
        "outputId": "b644c381-96f8-4b1a-d406-4ae248fab92a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "Batch 0/1875, Loss: 2.277789831161499\n",
            "Batch 100/1875, Loss: 0.48276323080062866\n",
            "Batch 200/1875, Loss: 0.37784743309020996\n",
            "Batch 300/1875, Loss: 0.27518677711486816\n",
            "Batch 400/1875, Loss: 0.23590126633644104\n",
            "Batch 500/1875, Loss: 0.40305736660957336\n",
            "Batch 600/1875, Loss: 0.17073597013950348\n",
            "Batch 700/1875, Loss: 0.16989339888095856\n",
            "Batch 800/1875, Loss: 0.15086308121681213\n",
            "Batch 900/1875, Loss: 0.159725159406662\n",
            "Batch 1000/1875, Loss: 0.3938857316970825\n",
            "Batch 1100/1875, Loss: 0.2124837338924408\n",
            "Batch 1200/1875, Loss: 0.20387053489685059\n",
            "Batch 1300/1875, Loss: 0.276106595993042\n",
            "Batch 1400/1875, Loss: 0.2740926742553711\n",
            "Batch 1500/1875, Loss: 0.1632634550333023\n",
            "Batch 1600/1875, Loss: 0.2519829273223877\n",
            "Batch 1700/1875, Loss: 0.11846309900283813\n",
            "Batch 1800/1875, Loss: 0.2113291323184967\n",
            "Epoch 2/5\n",
            "Batch 0/1875, Loss: 0.1133892685174942\n",
            "Batch 100/1875, Loss: 0.26501283049583435\n",
            "Batch 200/1875, Loss: 0.18137653172016144\n",
            "Batch 300/1875, Loss: 0.06045425683259964\n",
            "Batch 400/1875, Loss: 0.1105516254901886\n",
            "Batch 500/1875, Loss: 0.19048169255256653\n",
            "Batch 600/1875, Loss: 0.08891047537326813\n",
            "Batch 700/1875, Loss: 0.05957004055380821\n",
            "Batch 800/1875, Loss: 0.06767188012599945\n",
            "Batch 900/1875, Loss: 0.05642902851104736\n",
            "Batch 1000/1875, Loss: 0.25177332758903503\n",
            "Batch 1100/1875, Loss: 0.1728859394788742\n",
            "Batch 1200/1875, Loss: 0.10544107109308243\n",
            "Batch 1300/1875, Loss: 0.12105943262577057\n",
            "Batch 1400/1875, Loss: 0.13470160961151123\n",
            "Batch 1500/1875, Loss: 0.10901475697755814\n",
            "Batch 1600/1875, Loss: 0.202436164021492\n",
            "Batch 1700/1875, Loss: 0.03515748307108879\n",
            "Batch 1800/1875, Loss: 0.1302742063999176\n",
            "Epoch 3/5\n",
            "Batch 0/1875, Loss: 0.06741476058959961\n",
            "Batch 100/1875, Loss: 0.13652853667736053\n",
            "Batch 200/1875, Loss: 0.12939217686653137\n",
            "Batch 300/1875, Loss: 0.02621183544397354\n",
            "Batch 400/1875, Loss: 0.10348103940486908\n",
            "Batch 500/1875, Loss: 0.14368067681789398\n",
            "Batch 600/1875, Loss: 0.06433725357055664\n",
            "Batch 700/1875, Loss: 0.03156724572181702\n",
            "Batch 800/1875, Loss: 0.03841055929660797\n",
            "Batch 900/1875, Loss: 0.026483576744794846\n",
            "Batch 1000/1875, Loss: 0.2101670354604721\n",
            "Batch 1100/1875, Loss: 0.14396341145038605\n",
            "Batch 1200/1875, Loss: 0.07546161115169525\n",
            "Batch 1300/1875, Loss: 0.08635888248682022\n",
            "Batch 1400/1875, Loss: 0.06586291640996933\n",
            "Batch 1500/1875, Loss: 0.09143611788749695\n",
            "Batch 1600/1875, Loss: 0.14348477125167847\n",
            "Batch 1700/1875, Loss: 0.018836580216884613\n",
            "Batch 1800/1875, Loss: 0.08534573763608932\n",
            "Epoch 4/5\n",
            "Batch 0/1875, Loss: 0.04915757477283478\n",
            "Batch 100/1875, Loss: 0.0807487815618515\n",
            "Batch 200/1875, Loss: 0.10467611998319626\n",
            "Batch 300/1875, Loss: 0.017947092652320862\n",
            "Batch 400/1875, Loss: 0.09750224649906158\n",
            "Batch 500/1875, Loss: 0.11326908320188522\n",
            "Batch 600/1875, Loss: 0.03882543742656708\n",
            "Batch 700/1875, Loss: 0.02085857465863228\n",
            "Batch 800/1875, Loss: 0.02633085660636425\n",
            "Batch 900/1875, Loss: 0.015098377130925655\n",
            "Batch 1000/1875, Loss: 0.16851946711540222\n",
            "Batch 1100/1875, Loss: 0.11317166686058044\n",
            "Batch 1200/1875, Loss: 0.06273514032363892\n",
            "Batch 1300/1875, Loss: 0.06805267930030823\n",
            "Batch 1400/1875, Loss: 0.03698612004518509\n",
            "Batch 1500/1875, Loss: 0.06144113838672638\n",
            "Batch 1600/1875, Loss: 0.10305911302566528\n",
            "Batch 1700/1875, Loss: 0.01403737161308527\n",
            "Batch 1800/1875, Loss: 0.05713554099202156\n",
            "Epoch 5/5\n",
            "Batch 0/1875, Loss: 0.04114639759063721\n",
            "Batch 100/1875, Loss: 0.05416831374168396\n",
            "Batch 200/1875, Loss: 0.0833820253610611\n",
            "Batch 300/1875, Loss: 0.012185587547719479\n",
            "Batch 400/1875, Loss: 0.08940842747688293\n",
            "Batch 500/1875, Loss: 0.10188097506761551\n",
            "Batch 600/1875, Loss: 0.03190913051366806\n",
            "Batch 700/1875, Loss: 0.01572076790034771\n",
            "Batch 800/1875, Loss: 0.022795433178544044\n",
            "Batch 900/1875, Loss: 0.009862923994660378\n",
            "Batch 1000/1875, Loss: 0.14491811394691467\n",
            "Batch 1100/1875, Loss: 0.09374506771564484\n",
            "Batch 1200/1875, Loss: 0.057714734226465225\n",
            "Batch 1300/1875, Loss: 0.06261049211025238\n",
            "Batch 1400/1875, Loss: 0.019688399508595467\n",
            "Batch 1500/1875, Loss: 0.047981295734643936\n",
            "Batch 1600/1875, Loss: 0.07159362733364105\n",
            "Batch 1700/1875, Loss: 0.011492349207401276\n",
            "Batch 1800/1875, Loss: 0.03623111546039581\n",
            "Test Accuracy: 0.9693999886512756\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Root Mean Squared Propagation (RMSprop) from scatch"
      ],
      "metadata": {
        "id": "Tgq8dT5N6ZEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "\n",
        "# Load and preprocess data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize pixel values to range [0,1]\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Flatten images from 28x28 to 784-dimensional vectors\n",
        "x_train = x_train.reshape(-1, 28 * 28)\n",
        "x_test = x_test.reshape(-1, 28 * 28)\n",
        "\n",
        "# Convert labels to one-hot encoded format\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Build a simple feedforward neural network\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(28 * 28,)),  # Hidden layer with 128 neurons\n",
        "    Dense(10, activation='softmax')  # Output layer with 10 neurons (one for each digit)\n",
        "])\n",
        "\n",
        "class RMSProp:\n",
        "    \"\"\"\n",
        "    Custom implementation of the RMSProp optimizer.\n",
        "    RMSProp maintains an exponentially decaying average of past squared gradients\n",
        "    to normalize future updates.\n",
        "    \"\"\"\n",
        "    def __init__(self, learning_rate=0.001, rho=0.9, epsilon=1e-7):\n",
        "        \"\"\"\n",
        "        Initialize optimizer parameters.\n",
        "\n",
        "        Args:\n",
        "        learning_rate (float): Step size for parameter updates.\n",
        "        rho (float): Decay factor for the moving average of squared gradients.\n",
        "        epsilon (float): Small constant to prevent division by zero.\n",
        "        \"\"\"\n",
        "        self.lr = learning_rate\n",
        "        self.rho = rho\n",
        "        self.epsilon = epsilon\n",
        "        self.v = None  # Will store moving averages of squared gradients\n",
        "\n",
        "    def apply_gradients(self, grads, vars):\n",
        "        \"\"\"\n",
        "        Apply gradients to update model parameters using RMSProp.\n",
        "\n",
        "        Args:\n",
        "        grads (list of tensors): Computed gradients of the loss with respect to variables.\n",
        "        vars (list of tensors): Model parameters to be updated.\n",
        "        \"\"\"\n",
        "        if self.v is None:\n",
        "            # Initialize v as tf.Variables for proper assignment\n",
        "            self.v = [tf.Variable(tf.zeros_like(var)) for var in vars]\n",
        "\n",
        "        # Iterate over model parameters and gradients\n",
        "        for var, grad, v in zip(vars, grads, self.v):\n",
        "            if grad is None:\n",
        "                continue\n",
        "\n",
        "            # Update moving average of squared gradients\n",
        "            v.assign(self.rho * v + (1 - self.rho) * tf.square(grad))\n",
        "\n",
        "            # Update parameter values using RMSProp rule\n",
        "            var.assign_sub(self.lr * grad / (tf.sqrt(v) + self.epsilon))\n",
        "\n",
        "def train_model(model, x_train, y_train, epochs=5, batch_size=32):\n",
        "    \"\"\"\n",
        "    Train the model using mini-batch gradient descent with RMSProp.\n",
        "\n",
        "    Args:\n",
        "    model (Sequential): The neural network model.\n",
        "    x_train (numpy array): Training input data.\n",
        "    y_train (numpy array): Training labels (one-hot encoded).\n",
        "    epochs (int): Number of times the model sees the entire dataset.\n",
        "    batch_size (int): Number of samples per mini-batch.\n",
        "    \"\"\"\n",
        "    optimizer = RMSProp(learning_rate=0.001)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "\n",
        "        # Shuffle training data to ensure randomness in mini-batches\n",
        "        indices = np.random.permutation(len(x_train))\n",
        "        x_shuffled = x_train[indices]\n",
        "        y_shuffled = y_train[indices]\n",
        "\n",
        "        for batch in range(0, len(x_train), batch_size):\n",
        "            # Get mini-batch\n",
        "            x_batch = x_shuffled[batch:batch + batch_size]\n",
        "            y_batch = y_shuffled[batch:batch + batch_size]\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                # Forward pass: compute model predictions\n",
        "                preds = model(x_batch)\n",
        "\n",
        "                # Compute categorical cross-entropy loss\n",
        "                loss = tf.reduce_mean(\n",
        "                    tf.keras.losses.categorical_crossentropy(y_batch, preds)\n",
        "                )\n",
        "\n",
        "            # Compute gradients of the loss with respect to model parameters\n",
        "            grads = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "            # Apply gradients to update model parameters using RMSProp\n",
        "            optimizer.apply_gradients(grads, model.trainable_variables)\n",
        "\n",
        "            if batch % 100 == 0:\n",
        "                print(f\"Batch {batch}, Loss: {loss.numpy():.4f}\")\n",
        "\n",
        "def evaluate(model, x_test, y_test):\n",
        "    \"\"\"\n",
        "    Evaluate model performance on test data.\n",
        "\n",
        "    Args:\n",
        "    model (Sequential): The trained neural network model.\n",
        "    x_test (numpy array): Test input data.\n",
        "    y_test (numpy array): Test labels (one-hot encoded).\n",
        "    \"\"\"\n",
        "    preds = model(x_test)  # Get predictions\n",
        "\n",
        "    # Compute accuracy by comparing predicted and actual labels\n",
        "    accuracy = tf.reduce_mean(\n",
        "        tf.cast(\n",
        "            tf.equal(tf.argmax(y_test, axis=1),\n",
        "                     tf.argmax(preds, axis=1)),\n",
        "            tf.float32 ))\n",
        "\n",
        "    print(f\"Test Accuracy: {accuracy.numpy() * 100:.2f}%\")\n",
        "\n",
        "# Train and evaluate the model\n",
        "train_model(model, x_train, y_train)\n",
        "evaluate(model, x_test, y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BriN-3hi6Stm",
        "outputId": "52ebf193-e13d-40f7-f20f-247c73da8be0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "Batch 0, Loss: 2.5080\n",
            "Batch 800, Loss: 0.8016\n",
            "Batch 1600, Loss: 0.7601\n",
            "Batch 2400, Loss: 0.4361\n",
            "Batch 3200, Loss: 0.3866\n",
            "Batch 4000, Loss: 0.3057\n",
            "Batch 4800, Loss: 0.2728\n",
            "Batch 5600, Loss: 0.5450\n",
            "Batch 6400, Loss: 0.3420\n",
            "Batch 7200, Loss: 0.4329\n",
            "Batch 8000, Loss: 0.4170\n",
            "Batch 8800, Loss: 0.4125\n",
            "Batch 9600, Loss: 0.7629\n",
            "Batch 10400, Loss: 0.1626\n",
            "Batch 11200, Loss: 0.4347\n",
            "Batch 12000, Loss: 0.4849\n",
            "Batch 12800, Loss: 0.1728\n",
            "Batch 13600, Loss: 0.3058\n",
            "Batch 14400, Loss: 0.1866\n",
            "Batch 15200, Loss: 0.1389\n",
            "Batch 16000, Loss: 0.1698\n",
            "Batch 16800, Loss: 0.2578\n",
            "Batch 17600, Loss: 0.0932\n",
            "Batch 18400, Loss: 0.3345\n",
            "Batch 19200, Loss: 0.1821\n",
            "Batch 20000, Loss: 0.1327\n",
            "Batch 20800, Loss: 0.2788\n",
            "Batch 21600, Loss: 0.3314\n",
            "Batch 22400, Loss: 0.4195\n",
            "Batch 23200, Loss: 0.1408\n",
            "Batch 24000, Loss: 0.3518\n",
            "Batch 24800, Loss: 0.1454\n",
            "Batch 25600, Loss: 0.1775\n",
            "Batch 26400, Loss: 0.2199\n",
            "Batch 27200, Loss: 0.2454\n",
            "Batch 28000, Loss: 0.0895\n",
            "Batch 28800, Loss: 0.0976\n",
            "Batch 29600, Loss: 0.3020\n",
            "Batch 30400, Loss: 0.1332\n",
            "Batch 31200, Loss: 0.1892\n",
            "Batch 32000, Loss: 0.1621\n",
            "Batch 32800, Loss: 0.3537\n",
            "Batch 33600, Loss: 0.1463\n",
            "Batch 34400, Loss: 0.1453\n",
            "Batch 35200, Loss: 0.1453\n",
            "Batch 36000, Loss: 0.2467\n",
            "Batch 36800, Loss: 0.7509\n",
            "Batch 37600, Loss: 0.0937\n",
            "Batch 38400, Loss: 0.1461\n",
            "Batch 39200, Loss: 0.1459\n",
            "Batch 40000, Loss: 0.1747\n",
            "Batch 40800, Loss: 0.2199\n",
            "Batch 41600, Loss: 0.1823\n",
            "Batch 42400, Loss: 0.3706\n",
            "Batch 43200, Loss: 0.1716\n",
            "Batch 44000, Loss: 0.0715\n",
            "Batch 44800, Loss: 0.2391\n",
            "Batch 45600, Loss: 0.0190\n",
            "Batch 46400, Loss: 0.1143\n",
            "Batch 47200, Loss: 0.1262\n",
            "Batch 48000, Loss: 0.2987\n",
            "Batch 48800, Loss: 0.1952\n",
            "Batch 49600, Loss: 0.2991\n",
            "Batch 50400, Loss: 0.2216\n",
            "Batch 51200, Loss: 0.0537\n",
            "Batch 52000, Loss: 0.0260\n",
            "Batch 52800, Loss: 0.1347\n",
            "Batch 53600, Loss: 0.0681\n",
            "Batch 54400, Loss: 0.2438\n",
            "Batch 55200, Loss: 0.2707\n",
            "Batch 56000, Loss: 0.0116\n",
            "Batch 56800, Loss: 0.2733\n",
            "Batch 57600, Loss: 0.3166\n",
            "Batch 58400, Loss: 0.0766\n",
            "Batch 59200, Loss: 0.0597\n",
            "Epoch 2/5\n",
            "Batch 0, Loss: 0.2235\n",
            "Batch 800, Loss: 0.0359\n",
            "Batch 1600, Loss: 0.1588\n",
            "Batch 2400, Loss: 0.0799\n",
            "Batch 3200, Loss: 0.2236\n",
            "Batch 4000, Loss: 0.1974\n",
            "Batch 4800, Loss: 0.1073\n",
            "Batch 5600, Loss: 0.2457\n",
            "Batch 6400, Loss: 0.0332\n",
            "Batch 7200, Loss: 0.3307\n",
            "Batch 8000, Loss: 0.0645\n",
            "Batch 8800, Loss: 0.1253\n",
            "Batch 9600, Loss: 0.0464\n",
            "Batch 10400, Loss: 0.0194\n",
            "Batch 11200, Loss: 0.4745\n",
            "Batch 12000, Loss: 0.1161\n",
            "Batch 12800, Loss: 0.1882\n",
            "Batch 13600, Loss: 0.1140\n",
            "Batch 14400, Loss: 0.1129\n",
            "Batch 15200, Loss: 0.1534\n",
            "Batch 16000, Loss: 0.0330\n",
            "Batch 16800, Loss: 0.0894\n",
            "Batch 17600, Loss: 0.0286\n",
            "Batch 18400, Loss: 0.0574\n",
            "Batch 19200, Loss: 0.2233\n",
            "Batch 20000, Loss: 0.0585\n",
            "Batch 20800, Loss: 0.0298\n",
            "Batch 21600, Loss: 0.0701\n",
            "Batch 22400, Loss: 0.1042\n",
            "Batch 23200, Loss: 0.0895\n",
            "Batch 24000, Loss: 0.2676\n",
            "Batch 24800, Loss: 0.2612\n",
            "Batch 25600, Loss: 0.2108\n",
            "Batch 26400, Loss: 0.0564\n",
            "Batch 27200, Loss: 0.0148\n",
            "Batch 28000, Loss: 0.1359\n",
            "Batch 28800, Loss: 0.0081\n",
            "Batch 29600, Loss: 0.3387\n",
            "Batch 30400, Loss: 0.2201\n",
            "Batch 31200, Loss: 0.1143\n",
            "Batch 32000, Loss: 0.0084\n",
            "Batch 32800, Loss: 0.3992\n",
            "Batch 33600, Loss: 0.0334\n",
            "Batch 34400, Loss: 0.0492\n",
            "Batch 35200, Loss: 0.1132\n",
            "Batch 36000, Loss: 0.1852\n",
            "Batch 36800, Loss: 0.0330\n",
            "Batch 37600, Loss: 0.1702\n",
            "Batch 38400, Loss: 0.0103\n",
            "Batch 39200, Loss: 0.0336\n",
            "Batch 40000, Loss: 0.0356\n",
            "Batch 40800, Loss: 0.0341\n",
            "Batch 41600, Loss: 0.2149\n",
            "Batch 42400, Loss: 0.0668\n",
            "Batch 43200, Loss: 0.0255\n",
            "Batch 44000, Loss: 0.0174\n",
            "Batch 44800, Loss: 0.0372\n",
            "Batch 45600, Loss: 0.0263\n",
            "Batch 46400, Loss: 0.0922\n",
            "Batch 47200, Loss: 0.0656\n",
            "Batch 48000, Loss: 0.1619\n",
            "Batch 48800, Loss: 0.0341\n",
            "Batch 49600, Loss: 0.0863\n",
            "Batch 50400, Loss: 0.0351\n",
            "Batch 51200, Loss: 0.0395\n",
            "Batch 52000, Loss: 0.2220\n",
            "Batch 52800, Loss: 0.1223\n",
            "Batch 53600, Loss: 0.0155\n",
            "Batch 54400, Loss: 0.0231\n",
            "Batch 55200, Loss: 0.0396\n",
            "Batch 56000, Loss: 0.0325\n",
            "Batch 56800, Loss: 0.0140\n",
            "Batch 57600, Loss: 0.0349\n",
            "Batch 58400, Loss: 0.1357\n",
            "Batch 59200, Loss: 0.0864\n",
            "Epoch 3/5\n",
            "Batch 0, Loss: 0.1682\n",
            "Batch 800, Loss: 0.0216\n",
            "Batch 1600, Loss: 0.1834\n",
            "Batch 2400, Loss: 0.3500\n",
            "Batch 3200, Loss: 0.1182\n",
            "Batch 4000, Loss: 0.0315\n",
            "Batch 4800, Loss: 0.1278\n",
            "Batch 5600, Loss: 0.4143\n",
            "Batch 6400, Loss: 0.0765\n",
            "Batch 7200, Loss: 0.0584\n",
            "Batch 8000, Loss: 0.0539\n",
            "Batch 8800, Loss: 0.0312\n",
            "Batch 9600, Loss: 0.0148\n",
            "Batch 10400, Loss: 0.0997\n",
            "Batch 11200, Loss: 0.0796\n",
            "Batch 12000, Loss: 0.0521\n",
            "Batch 12800, Loss: 0.1830\n",
            "Batch 13600, Loss: 0.0436\n",
            "Batch 14400, Loss: 0.1429\n",
            "Batch 15200, Loss: 0.1265\n",
            "Batch 16000, Loss: 0.0345\n",
            "Batch 16800, Loss: 0.0639\n",
            "Batch 17600, Loss: 0.3577\n",
            "Batch 18400, Loss: 0.0518\n",
            "Batch 19200, Loss: 0.1483\n",
            "Batch 20000, Loss: 0.3363\n",
            "Batch 20800, Loss: 0.0767\n",
            "Batch 21600, Loss: 0.0366\n",
            "Batch 22400, Loss: 0.0755\n",
            "Batch 23200, Loss: 0.0599\n",
            "Batch 24000, Loss: 0.0567\n",
            "Batch 24800, Loss: 0.1312\n",
            "Batch 25600, Loss: 0.2006\n",
            "Batch 26400, Loss: 0.0622\n",
            "Batch 27200, Loss: 0.1600\n",
            "Batch 28000, Loss: 0.0081\n",
            "Batch 28800, Loss: 0.0321\n",
            "Batch 29600, Loss: 0.0112\n",
            "Batch 30400, Loss: 0.0061\n",
            "Batch 31200, Loss: 0.2362\n",
            "Batch 32000, Loss: 0.0212\n",
            "Batch 32800, Loss: 0.3484\n",
            "Batch 33600, Loss: 0.0734\n",
            "Batch 34400, Loss: 0.0221\n",
            "Batch 35200, Loss: 0.0289\n",
            "Batch 36000, Loss: 0.0722\n",
            "Batch 36800, Loss: 0.1014\n",
            "Batch 37600, Loss: 0.0179\n",
            "Batch 38400, Loss: 0.0402\n",
            "Batch 39200, Loss: 0.0255\n",
            "Batch 40000, Loss: 0.0073\n",
            "Batch 40800, Loss: 0.0248\n",
            "Batch 41600, Loss: 0.2395\n",
            "Batch 42400, Loss: 0.0288\n",
            "Batch 43200, Loss: 0.0250\n",
            "Batch 44000, Loss: 0.0109\n",
            "Batch 44800, Loss: 0.0047\n",
            "Batch 45600, Loss: 0.0200\n",
            "Batch 46400, Loss: 0.0415\n",
            "Batch 47200, Loss: 0.1180\n",
            "Batch 48000, Loss: 0.1979\n",
            "Batch 48800, Loss: 0.0151\n",
            "Batch 49600, Loss: 0.2352\n",
            "Batch 50400, Loss: 0.0668\n",
            "Batch 51200, Loss: 0.0812\n",
            "Batch 52000, Loss: 0.0555\n",
            "Batch 52800, Loss: 0.0435\n",
            "Batch 53600, Loss: 0.0294\n",
            "Batch 54400, Loss: 0.3087\n",
            "Batch 55200, Loss: 0.2195\n",
            "Batch 56000, Loss: 0.1762\n",
            "Batch 56800, Loss: 0.0050\n",
            "Batch 57600, Loss: 0.0504\n",
            "Batch 58400, Loss: 0.0394\n",
            "Batch 59200, Loss: 0.1277\n",
            "Epoch 4/5\n",
            "Batch 0, Loss: 0.1325\n",
            "Batch 800, Loss: 0.0199\n",
            "Batch 1600, Loss: 0.0106\n",
            "Batch 2400, Loss: 0.0605\n",
            "Batch 3200, Loss: 0.1009\n",
            "Batch 4000, Loss: 0.0100\n",
            "Batch 4800, Loss: 0.0126\n",
            "Batch 5600, Loss: 0.0190\n",
            "Batch 6400, Loss: 0.1528\n",
            "Batch 7200, Loss: 0.0100\n",
            "Batch 8000, Loss: 0.0316\n",
            "Batch 8800, Loss: 0.0716\n",
            "Batch 9600, Loss: 0.0247\n",
            "Batch 10400, Loss: 0.0129\n",
            "Batch 11200, Loss: 0.0136\n",
            "Batch 12000, Loss: 0.0855\n",
            "Batch 12800, Loss: 0.0174\n",
            "Batch 13600, Loss: 0.1415\n",
            "Batch 14400, Loss: 0.0066\n",
            "Batch 15200, Loss: 0.1643\n",
            "Batch 16000, Loss: 0.0134\n",
            "Batch 16800, Loss: 0.2483\n",
            "Batch 17600, Loss: 0.0151\n",
            "Batch 18400, Loss: 0.0646\n",
            "Batch 19200, Loss: 0.0787\n",
            "Batch 20000, Loss: 0.2766\n",
            "Batch 20800, Loss: 0.0052\n",
            "Batch 21600, Loss: 0.1295\n",
            "Batch 22400, Loss: 0.0219\n",
            "Batch 23200, Loss: 0.0039\n",
            "Batch 24000, Loss: 0.0606\n",
            "Batch 24800, Loss: 0.0058\n",
            "Batch 25600, Loss: 0.5985\n",
            "Batch 26400, Loss: 0.1394\n",
            "Batch 27200, Loss: 0.0271\n",
            "Batch 28000, Loss: 0.0255\n",
            "Batch 28800, Loss: 0.1905\n",
            "Batch 29600, Loss: 0.3264\n",
            "Batch 30400, Loss: 0.0024\n",
            "Batch 31200, Loss: 0.0087\n",
            "Batch 32000, Loss: 0.0262\n",
            "Batch 32800, Loss: 0.0087\n",
            "Batch 33600, Loss: 0.0021\n",
            "Batch 34400, Loss: 0.0311\n",
            "Batch 35200, Loss: 0.0058\n",
            "Batch 36000, Loss: 0.2517\n",
            "Batch 36800, Loss: 0.0741\n",
            "Batch 37600, Loss: 0.0280\n",
            "Batch 38400, Loss: 0.1960\n",
            "Batch 39200, Loss: 0.0410\n",
            "Batch 40000, Loss: 0.0103\n",
            "Batch 40800, Loss: 0.2882\n",
            "Batch 41600, Loss: 0.0862\n",
            "Batch 42400, Loss: 0.0134\n",
            "Batch 43200, Loss: 0.0199\n",
            "Batch 44000, Loss: 0.0402\n",
            "Batch 44800, Loss: 0.0043\n",
            "Batch 45600, Loss: 0.0322\n",
            "Batch 46400, Loss: 0.0161\n",
            "Batch 47200, Loss: 0.0169\n",
            "Batch 48000, Loss: 0.0084\n",
            "Batch 48800, Loss: 0.1079\n",
            "Batch 49600, Loss: 0.0447\n",
            "Batch 50400, Loss: 0.0827\n",
            "Batch 51200, Loss: 0.0187\n",
            "Batch 52000, Loss: 0.0372\n",
            "Batch 52800, Loss: 0.1115\n",
            "Batch 53600, Loss: 0.0208\n",
            "Batch 54400, Loss: 0.0046\n",
            "Batch 55200, Loss: 0.0548\n",
            "Batch 56000, Loss: 0.1313\n",
            "Batch 56800, Loss: 0.0333\n",
            "Batch 57600, Loss: 0.0344\n",
            "Batch 58400, Loss: 0.0416\n",
            "Batch 59200, Loss: 0.0580\n",
            "Epoch 5/5\n",
            "Batch 0, Loss: 0.3131\n",
            "Batch 800, Loss: 0.0088\n",
            "Batch 1600, Loss: 0.0206\n",
            "Batch 2400, Loss: 0.1121\n",
            "Batch 3200, Loss: 0.0353\n",
            "Batch 4000, Loss: 0.0316\n",
            "Batch 4800, Loss: 0.0383\n",
            "Batch 5600, Loss: 0.0227\n",
            "Batch 6400, Loss: 0.0099\n",
            "Batch 7200, Loss: 0.0623\n",
            "Batch 8000, Loss: 0.1287\n",
            "Batch 8800, Loss: 0.0046\n",
            "Batch 9600, Loss: 0.0125\n",
            "Batch 10400, Loss: 0.0065\n",
            "Batch 11200, Loss: 0.0162\n",
            "Batch 12000, Loss: 0.0528\n",
            "Batch 12800, Loss: 0.0087\n",
            "Batch 13600, Loss: 0.0540\n",
            "Batch 14400, Loss: 0.0072\n",
            "Batch 15200, Loss: 0.2480\n",
            "Batch 16000, Loss: 0.0029\n",
            "Batch 16800, Loss: 0.1533\n",
            "Batch 17600, Loss: 0.0112\n",
            "Batch 18400, Loss: 0.0268\n",
            "Batch 19200, Loss: 0.0023\n",
            "Batch 20000, Loss: 0.2163\n",
            "Batch 20800, Loss: 0.0072\n",
            "Batch 21600, Loss: 0.0887\n",
            "Batch 22400, Loss: 0.0716\n",
            "Batch 23200, Loss: 0.0264\n",
            "Batch 24000, Loss: 0.1036\n",
            "Batch 24800, Loss: 0.0046\n",
            "Batch 25600, Loss: 0.0063\n",
            "Batch 26400, Loss: 0.0183\n",
            "Batch 27200, Loss: 0.0100\n",
            "Batch 28000, Loss: 0.2611\n",
            "Batch 28800, Loss: 0.0093\n",
            "Batch 29600, Loss: 0.0625\n",
            "Batch 30400, Loss: 0.0288\n",
            "Batch 31200, Loss: 0.0078\n",
            "Batch 32000, Loss: 0.0532\n",
            "Batch 32800, Loss: 0.0023\n",
            "Batch 33600, Loss: 0.0083\n",
            "Batch 34400, Loss: 0.0150\n",
            "Batch 35200, Loss: 0.0016\n",
            "Batch 36000, Loss: 0.0348\n",
            "Batch 36800, Loss: 0.0186\n",
            "Batch 37600, Loss: 0.0125\n",
            "Batch 38400, Loss: 0.0029\n",
            "Batch 39200, Loss: 0.0064\n",
            "Batch 40000, Loss: 0.0050\n",
            "Batch 40800, Loss: 0.0138\n",
            "Batch 41600, Loss: 0.0078\n",
            "Batch 42400, Loss: 0.0036\n",
            "Batch 43200, Loss: 0.0039\n",
            "Batch 44000, Loss: 0.1445\n",
            "Batch 44800, Loss: 0.1015\n",
            "Batch 45600, Loss: 0.0215\n",
            "Batch 46400, Loss: 0.0029\n",
            "Batch 47200, Loss: 0.0310\n",
            "Batch 48000, Loss: 0.0056\n",
            "Batch 48800, Loss: 0.0011\n",
            "Batch 49600, Loss: 0.0230\n",
            "Batch 50400, Loss: 0.1360\n",
            "Batch 51200, Loss: 0.0269\n",
            "Batch 52000, Loss: 0.0306\n",
            "Batch 52800, Loss: 0.0301\n",
            "Batch 53600, Loss: 0.0118\n",
            "Batch 54400, Loss: 0.0635\n",
            "Batch 55200, Loss: 0.0329\n",
            "Batch 56000, Loss: 0.0958\n",
            "Batch 56800, Loss: 0.0115\n",
            "Batch 57600, Loss: 0.0324\n",
            "Batch 58400, Loss: 0.0050\n",
            "Batch 59200, Loss: 0.0092\n",
            "Test Accuracy: 97.39%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Adaptive Moment Estimation (Adam) from scratch"
      ],
      "metadata": {
        "id": "Z6ASVJUc74Jd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "\n",
        "# Load and preprocess MNIST dataset of handwritten digits\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize pixel values\n",
        "x_train = x_train.reshape(-1, 28 * 28)  # Flatten 28x28 images to 784-dim vectors\n",
        "x_test = x_test.reshape(-1, 28 * 28)\n",
        "y_train = to_categorical(y_train, 10)  # Convert labels to one-hot encoding\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Build neural network model\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(28 * 28,)), #relu introduces non-linearity\n",
        "    Dense(10, activation='softmax')])  # Output layer with probability distribution\n",
        "\n",
        "class CustomAdam:\n",
        "    \"\"\"\n",
        "    Custom implementation of Adam (Adaptive Moment Estimation) optimizer\n",
        "    Combines benefits of RMSProp (adaptive learning rates) and momentum\n",
        "\n",
        "    Key components:\n",
        "    - m: First moment vector (mean of gradients)\n",
        "    - v: Second moment vector (uncentered variance of gradients)\n",
        "    - beta1: Exponential decay rate for first moment (typically 0.9)\n",
        "    - beta2: Exponential decay rate for second moment (typically 0.999)\n",
        "    - epsilon: Small constant for numerical stability\n",
        "    - t: Time step counter for bias correction\n",
        "    \"\"\"\n",
        "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-7):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "        self.m = None  # Will store first moment estimates\n",
        "        self.v = None  # Will store second moment estimates\n",
        "        self.t = 0  # Iteration counter\n",
        "\n",
        "    def update(self, weights, grads):\n",
        "        \"\"\"Update weights using Adam optimization algorithm\"\"\"\n",
        "        # Initialize moment vectors on first update\n",
        "        if self.m is None:\n",
        "            # Initialize m and v as lists of tf.Variables\n",
        "            self.m = [tf.Variable(tf.zeros_like(w)) for w in weights]\n",
        "            self.v = [tf.Variable(tf.zeros_like(w)) for w in weights]\n",
        "\n",
        "        self.t += 1  # Increment time step\n",
        "\n",
        "        for i in range(len(weights)):\n",
        "            # Update biased first moment estimate (like momentum)\n",
        "            self.m[i].assign(self.beta1 * self.m[i] + (1 - self.beta1) * grads[i])\n",
        "\n",
        "            # Update biased second moment estimate (like RMSProp)\n",
        "            self.v[i].assign(self.beta2 * self.v[i] + (1 - self.beta2) * tf.square(grads[i]))\n",
        "\n",
        "            # Compute bias-corrected first moment estimate\n",
        "            m_hat = self.m[i] / (1 - tf.pow(self.beta1, self.t))\n",
        "\n",
        "            # Compute bias-corrected second moment estimate\n",
        "            v_hat = self.v[i] / (1 - tf.pow(self.beta2, self.t))\n",
        "\n",
        "            # Update parameters with adaptive learning rate\n",
        "            weights[i].assign_sub(self.learning_rate * m_hat / (tf.sqrt(v_hat) + self.epsilon))\n",
        "\n",
        "# Loss function for multi-class classification\n",
        "def categorical_crossentropy(y_true, y_pred):\n",
        "    \"\"\"Computes cross-entropy between true labels and predicted probabilities\"\"\"\n",
        "    return -tf.reduce_mean(tf.reduce_sum(y_true * tf.math.log(y_pred), axis=1))\n",
        "\n",
        "def train_model(model, x_train, y_train, epochs=5, batch_size=32, learning_rate=0.001):\n",
        "    \"\"\"Training loop with custom Adam optimizer\"\"\"\n",
        "    optimizer = CustomAdam(learning_rate=learning_rate)\n",
        "    num_samples = x_train.shape[0]\n",
        "    num_batches = int(np.ceil(num_samples / batch_size))\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "        epoch_loss = []\n",
        "\n",
        "        for batch in range(num_batches):\n",
        "            # Get batch data\n",
        "            start = batch * batch_size\n",
        "            end = start + batch_size\n",
        "            x_batch = x_train[start:end]\n",
        "            y_batch = y_train[start:end]\n",
        "\n",
        "            # Forward pass with gradient tracking\n",
        "            with tf.GradientTape() as tape:\n",
        "                y_pred = model(x_batch, training=True)\n",
        "                loss = categorical_crossentropy(y_batch, y_pred)\n",
        "\n",
        "            # Compute gradients\n",
        "            grads = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "            # Update weights using Adam\n",
        "            optimizer.update(model.trainable_variables, grads)\n",
        "\n",
        "            # Track loss\n",
        "            epoch_loss.append(loss.numpy())\n",
        "\n",
        "            # Print progress\n",
        "            if batch % 100 == 0:\n",
        "                print(f\"Batch {batch}/{num_batches}, Loss: {loss.numpy():.4f}\")\n",
        "\n",
        "        print(f\"Epoch Avg Loss: {np.mean(epoch_loss):.4f}\")\n",
        "\n",
        "# Train the model\n",
        "train_model(model, x_train, y_train)\n",
        "\n",
        "def evaluate_model(model, x_test, y_test):\n",
        "    \"\"\"Evaluate model accuracy on test set\"\"\"\n",
        "    y_pred = model(x_test)\n",
        "    y_pred = tf.argmax(y_pred, axis=1)  # Convert probabilities to class indices\n",
        "    y_true = tf.argmax(y_test, axis=1)  # Convert one-hot to class indices\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(y_pred, y_true), tf.float32))\n",
        "    print(f\"\\nTest Accuracy: {accuracy.numpy()*100:.2f}%\")\n",
        "\n",
        "# Evaluate model performance\n",
        "evaluate_model(model, x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uc-pOzUV6-ld",
        "outputId": "754d6ed2-43ab-4308-cb32-274d9376ca03"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "Batch 0/1875, Loss: 2.4785\n",
            "Batch 100/1875, Loss: 0.5396\n",
            "Batch 200/1875, Loss: 0.3674\n",
            "Batch 300/1875, Loss: 0.2544\n",
            "Batch 400/1875, Loss: 0.1693\n",
            "Batch 500/1875, Loss: 0.3770\n",
            "Batch 600/1875, Loss: 0.1740\n",
            "Batch 700/1875, Loss: 0.1249\n",
            "Batch 800/1875, Loss: 0.2094\n",
            "Batch 900/1875, Loss: 0.1223\n",
            "Batch 1000/1875, Loss: 0.4019\n",
            "Batch 1100/1875, Loss: 0.2129\n",
            "Batch 1200/1875, Loss: 0.1556\n",
            "Batch 1300/1875, Loss: 0.2041\n",
            "Batch 1400/1875, Loss: 0.2790\n",
            "Batch 1500/1875, Loss: 0.1339\n",
            "Batch 1600/1875, Loss: 0.2121\n",
            "Batch 1700/1875, Loss: 0.0936\n",
            "Batch 1800/1875, Loss: 0.2057\n",
            "Epoch Avg Loss: 0.2716\n",
            "Epoch 2/5\n",
            "Batch 0/1875, Loss: 0.0635\n",
            "Batch 100/1875, Loss: 0.2010\n",
            "Batch 200/1875, Loss: 0.1275\n",
            "Batch 300/1875, Loss: 0.0388\n",
            "Batch 400/1875, Loss: 0.0953\n",
            "Batch 500/1875, Loss: 0.1513\n",
            "Batch 600/1875, Loss: 0.0440\n",
            "Batch 700/1875, Loss: 0.0449\n",
            "Batch 800/1875, Loss: 0.1037\n",
            "Batch 900/1875, Loss: 0.0372\n",
            "Batch 1000/1875, Loss: 0.2292\n",
            "Batch 1100/1875, Loss: 0.1453\n",
            "Batch 1200/1875, Loss: 0.0806\n",
            "Batch 1300/1875, Loss: 0.1048\n",
            "Batch 1400/1875, Loss: 0.2107\n",
            "Batch 1500/1875, Loss: 0.0659\n",
            "Batch 1600/1875, Loss: 0.1745\n",
            "Batch 1700/1875, Loss: 0.0330\n",
            "Batch 1800/1875, Loss: 0.0911\n",
            "Epoch Avg Loss: 0.1244\n",
            "Epoch 3/5\n",
            "Batch 0/1875, Loss: 0.0372\n",
            "Batch 100/1875, Loss: 0.1041\n",
            "Batch 200/1875, Loss: 0.1200\n",
            "Batch 300/1875, Loss: 0.0216\n",
            "Batch 400/1875, Loss: 0.0964\n",
            "Batch 500/1875, Loss: 0.0764\n",
            "Batch 600/1875, Loss: 0.0474\n",
            "Batch 700/1875, Loss: 0.0201\n",
            "Batch 800/1875, Loss: 0.0590\n",
            "Batch 900/1875, Loss: 0.0199\n",
            "Batch 1000/1875, Loss: 0.1412\n",
            "Batch 1100/1875, Loss: 0.0808\n",
            "Batch 1200/1875, Loss: 0.0590\n",
            "Batch 1300/1875, Loss: 0.0733\n",
            "Batch 1400/1875, Loss: 0.1075\n",
            "Batch 1500/1875, Loss: 0.0391\n",
            "Batch 1600/1875, Loss: 0.1097\n",
            "Batch 1700/1875, Loss: 0.0200\n",
            "Batch 1800/1875, Loss: 0.0456\n",
            "Epoch Avg Loss: 0.0841\n",
            "Epoch 4/5\n",
            "Batch 0/1875, Loss: 0.0317\n",
            "Batch 100/1875, Loss: 0.0655\n",
            "Batch 200/1875, Loss: 0.1177\n",
            "Batch 300/1875, Loss: 0.0213\n",
            "Batch 400/1875, Loss: 0.1075\n",
            "Batch 500/1875, Loss: 0.0639\n",
            "Batch 600/1875, Loss: 0.0623\n",
            "Batch 700/1875, Loss: 0.0122\n",
            "Batch 800/1875, Loss: 0.0351\n",
            "Batch 900/1875, Loss: 0.0173\n",
            "Batch 1000/1875, Loss: 0.1265\n",
            "Batch 1100/1875, Loss: 0.0579\n",
            "Batch 1200/1875, Loss: 0.0506\n",
            "Batch 1300/1875, Loss: 0.0707\n",
            "Batch 1400/1875, Loss: 0.0699\n",
            "Batch 1500/1875, Loss: 0.0172\n",
            "Batch 1600/1875, Loss: 0.0550\n",
            "Batch 1700/1875, Loss: 0.0146\n",
            "Batch 1800/1875, Loss: 0.0302\n",
            "Epoch Avg Loss: 0.0612\n",
            "Epoch 5/5\n",
            "Batch 0/1875, Loss: 0.0239\n",
            "Batch 100/1875, Loss: 0.0338\n",
            "Batch 200/1875, Loss: 0.0994\n",
            "Batch 300/1875, Loss: 0.0156\n",
            "Batch 400/1875, Loss: 0.0809\n",
            "Batch 500/1875, Loss: 0.0448\n",
            "Batch 600/1875, Loss: 0.0557\n",
            "Batch 700/1875, Loss: 0.0091\n",
            "Batch 800/1875, Loss: 0.0279\n",
            "Batch 900/1875, Loss: 0.0150\n",
            "Batch 1000/1875, Loss: 0.1028\n",
            "Batch 1100/1875, Loss: 0.0243\n",
            "Batch 1200/1875, Loss: 0.0413\n",
            "Batch 1300/1875, Loss: 0.0655\n",
            "Batch 1400/1875, Loss: 0.0251\n",
            "Batch 1500/1875, Loss: 0.0182\n",
            "Batch 1600/1875, Loss: 0.0294\n",
            "Batch 1700/1875, Loss: 0.0170\n",
            "Batch 1800/1875, Loss: 0.0212\n",
            "Epoch Avg Loss: 0.0450\n",
            "\n",
            "Test Accuracy: 97.09%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Stochastic Gradient Descent (SGD) using tensorflow"
      ],
      "metadata": {
        "id": "JtpKNeW28_A-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load and preprocess data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize pixel values\n",
        "x_train = x_train.reshape(-1, 28 * 28)  # Flatten images\n",
        "x_test = x_test.reshape(-1, 28 * 28)\n",
        "y_train = to_categorical(y_train, 10)  # One-hot encode labels\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Create and compile the model\n",
        "model = Sequential([Dense(128, activation='relu', input_shape=(28 * 28,)), Dense(10, activation='softmax')])\n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train and evaluate the model\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=0.2)\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(\"Test accuracy:\",test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hCiTvhQ77-k",
        "outputId": "c1fe46bf-ee2e-4184-a788-54e8a8418781"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7056 - loss: 1.1232 - val_accuracy: 0.9012 - val_loss: 0.3806\n",
            "Epoch 2/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8968 - loss: 0.3835 - val_accuracy: 0.9130 - val_loss: 0.3138\n",
            "Epoch 3/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.9109 - loss: 0.3187 - val_accuracy: 0.9227 - val_loss: 0.2771\n",
            "Epoch 4/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9225 - loss: 0.2769 - val_accuracy: 0.9287 - val_loss: 0.2545\n",
            "Epoch 5/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9259 - loss: 0.2620 - val_accuracy: 0.9348 - val_loss: 0.2364\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9218 - loss: 0.2751\n",
            "Test accuracy: 0.9333999752998352\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. SGD with momentum using tensorflow"
      ],
      "metadata": {
        "id": "k-RL3dyK9AGE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import SGD  # Import SGD optimizer\n",
        "\n",
        "# Load and preprocess data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize pixel values\n",
        "x_train = x_train.reshape(-1, 28 * 28)  # Flatten images\n",
        "x_test = x_test.reshape(-1, 28 * 28)\n",
        "y_train = to_categorical(y_train, 10)  # One-hot encode labels\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Create and compile the model with SGD with Momentum\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(28 * 28,)),  # Hidden layer\n",
        "    Dense(10, activation='softmax')  # Output layer\n",
        "])\n",
        "\n",
        "# Configure SGD with Momentum (momentum=0.9 is a common value)\n",
        "sgd_with_momentum = SGD(learning_rate=0.01, momentum=0.9)\n",
        "model.compile(optimizer=sgd_with_momentum,\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train and evaluate the model\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=5,\n",
        "                    batch_size=32,\n",
        "                    validation_split=0.2)\n",
        "\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(\"Test accuracy:\", test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVnRbwA79A0H",
        "outputId": "af2812d0-300b-49d3-ee54-d48d4e03f10a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8362 - loss: 0.5568 - val_accuracy: 0.9482 - val_loss: 0.1857\n",
            "Epoch 2/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9484 - loss: 0.1747 - val_accuracy: 0.9617 - val_loss: 0.1370\n",
            "Epoch 3/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9658 - loss: 0.1179 - val_accuracy: 0.9672 - val_loss: 0.1127\n",
            "Epoch 4/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.9739 - loss: 0.0932 - val_accuracy: 0.9692 - val_loss: 0.1057\n",
            "Epoch 5/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9781 - loss: 0.0764 - val_accuracy: 0.9714 - val_loss: 0.0953\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9682 - loss: 0.1009\n",
            "Test accuracy: 0.9732999801635742\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Adaptive Moment Estimation (Adam) using tensorflow"
      ],
      "metadata": {
        "id": "zM9SJLQ09BU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "\n",
        "# Load and preprocess data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize pixel values\n",
        "x_train = x_train.reshape(-1, 28 * 28)  # Flatten images\n",
        "x_test = x_test.reshape(-1, 28 * 28)\n",
        "y_train = to_categorical(y_train, 10)  # One-hot encode labels\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Create and compile the model\n",
        "model = Sequential([Dense(128, activation='relu', input_shape=(28 * 28,)), Dense(10, activation='softmax')])\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train and evaluate the model\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=0.2)\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(\"Test accuracy:\",test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SR5-rYYH9Bws",
        "outputId": "0434f694-0d30-4f7b-d9b7-d751728e0e1a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.8680 - loss: 0.4692 - val_accuracy: 0.9513 - val_loss: 0.1696\n",
            "Epoch 2/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.9575 - loss: 0.1434 - val_accuracy: 0.9678 - val_loss: 0.1128\n",
            "Epoch 3/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9711 - loss: 0.0962 - val_accuracy: 0.9707 - val_loss: 0.1004\n",
            "Epoch 4/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.9805 - loss: 0.0683 - val_accuracy: 0.9703 - val_loss: 0.0978\n",
            "Epoch 5/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.9851 - loss: 0.0499 - val_accuracy: 0.9736 - val_loss: 0.0921\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9708 - loss: 0.0935\n",
            "Test accuracy: 0.9740999937057495\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Root Mean Squared Propagation (RMSprop) using tensorflow"
      ],
      "metadata": {
        "id": "0n7TtYUT9CGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load and preprocess data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize pixel values\n",
        "x_train = x_train.reshape(-1, 28 * 28)  # Flatten images\n",
        "x_test = x_test.reshape(-1, 28 * 28)\n",
        "y_train = to_categorical(y_train, 10)  # One-hot encode labels\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Create and compile the model\n",
        "model = Sequential([Dense(128, activation='relu', input_shape=(28 * 28,)), Dense(10, activation='softmax')])\n",
        "model.compile(optimizer='rmsprop' ,loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train and evaluate the model\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=0.2)\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(\"Test accuracy:\",test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OANNyz5M9Cko",
        "outputId": "3eb19dd2-cdc4-4b1e-8d03-18aff9ee273d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.8719 - loss: 0.4611 - val_accuracy: 0.9520 - val_loss: 0.1629\n",
            "Epoch 2/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9584 - loss: 0.1420 - val_accuracy: 0.9653 - val_loss: 0.1200\n",
            "Epoch 3/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.9717 - loss: 0.0938 - val_accuracy: 0.9679 - val_loss: 0.1073\n",
            "Epoch 4/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9786 - loss: 0.0728 - val_accuracy: 0.9732 - val_loss: 0.0926\n",
            "Epoch 5/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.9828 - loss: 0.0604 - val_accuracy: 0.9737 - val_loss: 0.0921\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9717 - loss: 0.1041\n",
            "Test accuracy: 0.9749000072479248\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "shlygFqo-g0s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}