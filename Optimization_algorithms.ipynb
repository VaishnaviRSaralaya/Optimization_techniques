{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da1f4325-726f-4f99-a1ba-f6248e310a14",
   "metadata": {},
   "source": [
    "Optimization Algorithms from scratch implementations and library implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029fc138-f7ca-4d87-aead-55dc8c653fc4",
   "metadata": {},
   "source": [
    "1.Stochastic Gradient Descent (SGD) from scatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9423d408-d519-446c-b641-b9834d9b5b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Batch 0/1875, Loss: 2.3270652294158936\n",
      "Batch 100/1875, Loss: 1.6160459518432617\n",
      "Batch 200/1875, Loss: 1.0054333209991455\n",
      "Batch 300/1875, Loss: 0.8898893594741821\n",
      "Batch 400/1875, Loss: 0.8000794649124146\n",
      "Batch 500/1875, Loss: 0.8378880023956299\n",
      "Batch 600/1875, Loss: 0.5260149836540222\n",
      "Batch 700/1875, Loss: 0.5218067765235901\n",
      "Batch 800/1875, Loss: 0.40523576736450195\n",
      "Batch 900/1875, Loss: 0.34206241369247437\n",
      "Batch 1000/1875, Loss: 0.5301038026809692\n",
      "Batch 1100/1875, Loss: 0.43280118703842163\n",
      "Batch 1200/1875, Loss: 0.44740551710128784\n",
      "Batch 1300/1875, Loss: 0.48487603664398193\n",
      "Batch 1400/1875, Loss: 0.368030309677124\n",
      "Batch 1500/1875, Loss: 0.2723538875579834\n",
      "Batch 1600/1875, Loss: 0.3821481466293335\n",
      "Batch 1700/1875, Loss: 0.2665557861328125\n",
      "Batch 1800/1875, Loss: 0.38760656118392944\n",
      "Epoch 2/5\n",
      "Batch 0/1875, Loss: 0.4517681896686554\n",
      "Batch 100/1875, Loss: 0.3804982304573059\n",
      "Batch 200/1875, Loss: 0.379755437374115\n",
      "Batch 300/1875, Loss: 0.29828667640686035\n",
      "Batch 400/1875, Loss: 0.2869582772254944\n",
      "Batch 500/1875, Loss: 0.4529730975627899\n",
      "Batch 600/1875, Loss: 0.28116297721862793\n",
      "Batch 700/1875, Loss: 0.24837090075016022\n",
      "Batch 800/1875, Loss: 0.2262294441461563\n",
      "Batch 900/1875, Loss: 0.17671085894107819\n",
      "Batch 1000/1875, Loss: 0.42751026153564453\n",
      "Batch 1100/1875, Loss: 0.29575595259666443\n",
      "Batch 1200/1875, Loss: 0.3236905336380005\n",
      "Batch 1300/1875, Loss: 0.3564509153366089\n",
      "Batch 1400/1875, Loss: 0.2880059480667114\n",
      "Batch 1500/1875, Loss: 0.20451976358890533\n",
      "Batch 1600/1875, Loss: 0.29997488856315613\n",
      "Batch 1700/1875, Loss: 0.1852753758430481\n",
      "Batch 1800/1875, Loss: 0.3449369966983795\n",
      "Epoch 3/5\n",
      "Batch 0/1875, Loss: 0.3306964039802551\n",
      "Batch 100/1875, Loss: 0.33484843373298645\n",
      "Batch 200/1875, Loss: 0.3365078568458557\n",
      "Batch 300/1875, Loss: 0.225988507270813\n",
      "Batch 400/1875, Loss: 0.22328004240989685\n",
      "Batch 500/1875, Loss: 0.3736794590950012\n",
      "Batch 600/1875, Loss: 0.21441924571990967\n",
      "Batch 700/1875, Loss: 0.18186195194721222\n",
      "Batch 800/1875, Loss: 0.17198942601680756\n",
      "Batch 900/1875, Loss: 0.1314217746257782\n",
      "Batch 1000/1875, Loss: 0.3980364501476288\n",
      "Batch 1100/1875, Loss: 0.25655245780944824\n",
      "Batch 1200/1875, Loss: 0.2736435532569885\n",
      "Batch 1300/1875, Loss: 0.29274293780326843\n",
      "Batch 1400/1875, Loss: 0.25756990909576416\n",
      "Batch 1500/1875, Loss: 0.17951370775699615\n",
      "Batch 1600/1875, Loss: 0.25853970646858215\n",
      "Batch 1700/1875, Loss: 0.14506691694259644\n",
      "Batch 1800/1875, Loss: 0.3156369924545288\n",
      "Epoch 4/5\n",
      "Batch 0/1875, Loss: 0.2675214111804962\n",
      "Batch 100/1875, Loss: 0.3194386065006256\n",
      "Batch 200/1875, Loss: 0.3006339371204376\n",
      "Batch 300/1875, Loss: 0.18432775139808655\n",
      "Batch 400/1875, Loss: 0.19281278550624847\n",
      "Batch 500/1875, Loss: 0.3343549966812134\n",
      "Batch 600/1875, Loss: 0.1788427084684372\n",
      "Batch 700/1875, Loss: 0.14649081230163574\n",
      "Batch 800/1875, Loss: 0.1443694829940796\n",
      "Batch 900/1875, Loss: 0.11105673760175705\n",
      "Batch 1000/1875, Loss: 0.3837921619415283\n",
      "Batch 1100/1875, Loss: 0.2336914837360382\n",
      "Batch 1200/1875, Loss: 0.23915663361549377\n",
      "Batch 1300/1875, Loss: 0.2507968842983246\n",
      "Batch 1400/1875, Loss: 0.23962156474590302\n",
      "Batch 1500/1875, Loss: 0.16236670315265656\n",
      "Batch 1600/1875, Loss: 0.2241417020559311\n",
      "Batch 1700/1875, Loss: 0.11897513270378113\n",
      "Batch 1800/1875, Loss: 0.2932955324649811\n",
      "Epoch 5/5\n",
      "Batch 0/1875, Loss: 0.22601409256458282\n",
      "Batch 100/1875, Loss: 0.3070043921470642\n",
      "Batch 200/1875, Loss: 0.27184364199638367\n",
      "Batch 300/1875, Loss: 0.15562132000923157\n",
      "Batch 400/1875, Loss: 0.1755150407552719\n",
      "Batch 500/1875, Loss: 0.3066045343875885\n",
      "Batch 600/1875, Loss: 0.1566724330186844\n",
      "Batch 700/1875, Loss: 0.12357115000486374\n",
      "Batch 800/1875, Loss: 0.12846998870372772\n",
      "Batch 900/1875, Loss: 0.0982065200805664\n",
      "Batch 1000/1875, Loss: 0.37438511848449707\n",
      "Batch 1100/1875, Loss: 0.21938903629779816\n",
      "Batch 1200/1875, Loss: 0.21074101328849792\n",
      "Batch 1300/1875, Loss: 0.22084954380989075\n",
      "Batch 1400/1875, Loss: 0.2232726812362671\n",
      "Batch 1500/1875, Loss: 0.15072938799858093\n",
      "Batch 1600/1875, Loss: 0.19747263193130493\n",
      "Batch 1700/1875, Loss: 0.10153896361589432\n",
      "Batch 1800/1875, Loss: 0.27686893939971924\n",
      "Test Accuracy: 0.935699999332428\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "# Load and preprocess data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize pixel values to [0, 1]\n",
    "x_train = x_train.reshape(-1, 28 * 28)  # Flatten the images\n",
    "x_test = x_test.reshape(-1, 28 * 28)\n",
    "y_train = to_categorical(y_train, 10)  # One-hot encode labels\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Create the model\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(28 * 28,)),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Define a custom optimizer from scratch\n",
    "class CustomSGD:\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def update(self, weights, grads):\n",
    "        \"\"\"\n",
    "        Update weights using gradient descent.\n",
    "        :param weights: List of model weights (trainable variables).\n",
    "        :param grads: List of gradients corresponding to the weights.\n",
    "        \"\"\"\n",
    "        for i in range(len(weights)):\n",
    "            weights[i].assign_sub(self.learning_rate * grads[i])  # w = w - lr * grad\n",
    "\n",
    "# Loss function (categorical cross-entropy)\n",
    "def categorical_crossentropy(y_true, y_pred):\n",
    "    return -tf.reduce_mean(tf.reduce_sum(y_true * tf.math.log(y_pred), axis=1))\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, x_train, y_train, epochs=5, batch_size=32, learning_rate=0.01):\n",
    "    optimizer = CustomSGD(learning_rate=learning_rate)\n",
    "    num_samples = x_train.shape[0]\n",
    "    num_batches = int(np.ceil(num_samples / batch_size))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        for batch in range(num_batches):\n",
    "            # Get a batch of data\n",
    "            start = batch * batch_size\n",
    "            end = start + batch_size\n",
    "            x_batch = x_train[start:end]\n",
    "            y_batch = y_train[start:end]\n",
    "\n",
    "            # Forward pass\n",
    "            with tf.GradientTape() as tape:\n",
    "                y_pred = model(x_batch, training=True)\n",
    "                loss = categorical_crossentropy(y_batch, y_pred)\n",
    "\n",
    "            # Backward pass (compute gradients)\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "            # Update weights using the custom optimizer\n",
    "            optimizer.update(model.trainable_variables, grads)\n",
    "\n",
    "            # Print loss every 100 batches\n",
    "            if batch % 100 == 0:\n",
    "                print(f\"Batch {batch}/{num_batches}, Loss: {loss.numpy()}\")\n",
    "\n",
    "# Train the model\n",
    "train_model(model, x_train, y_train, epochs=5, batch_size=32, learning_rate=0.01)\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate_model(model, x_test, y_test):\n",
    "    y_pred = model(x_test)\n",
    "    y_pred = tf.argmax(y_pred, axis=1)\n",
    "    y_true = tf.argmax(y_test, axis=1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(y_pred, y_true), tf.float32))\n",
    "    print(f\"Test Accuracy: {accuracy.numpy()}\")\n",
    "\n",
    "evaluate_model(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39592a3a-040d-4417-bc06-0c581fa6919d",
   "metadata": {},
   "source": [
    "2. Adaptive Moment Estimation (Adam) from scatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf583f6-9411-468e-ae9e-ce536b7e9631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Batch 0/1875, Loss: 2.4011390209198\n",
      "Batch 100/1875, Loss: 0.4783175587654114\n",
      "Batch 200/1875, Loss: 0.34606945514678955\n",
      "Batch 300/1875, Loss: 0.1497497260570526\n",
      "Batch 400/1875, Loss: 0.1784745305776596\n",
      "Batch 500/1875, Loss: 0.31149929761886597\n",
      "Batch 600/1875, Loss: 0.28456440567970276\n",
      "Batch 700/1875, Loss: 0.13499826192855835\n",
      "Batch 800/1875, Loss: 0.18486712872982025\n",
      "Batch 900/1875, Loss: 0.13458126783370972\n",
      "Batch 1000/1875, Loss: 0.461059033870697\n",
      "Batch 1100/1875, Loss: 0.28478243947029114\n",
      "Batch 1200/1875, Loss: 0.15832790732383728\n",
      "Batch 1300/1875, Loss: 0.1299748420715332\n",
      "Batch 1400/1875, Loss: 0.16376101970672607\n",
      "Batch 1500/1875, Loss: 0.1700705885887146\n",
      "Batch 1600/1875, Loss: 0.14414790272712708\n",
      "Batch 1700/1875, Loss: 0.06671467423439026\n",
      "Batch 1800/1875, Loss: 0.1317974030971527\n",
      "Epoch 2/5\n",
      "Batch 0/1875, Loss: 0.1034427285194397\n",
      "Batch 100/1875, Loss: 0.1183730885386467\n",
      "Batch 200/1875, Loss: 0.17879898846149445\n",
      "Batch 300/1875, Loss: 0.018749509006738663\n",
      "Batch 400/1875, Loss: 0.07359258085489273\n",
      "Batch 500/1875, Loss: 0.18322274088859558\n",
      "Batch 600/1875, Loss: 0.06755746155977249\n",
      "Batch 700/1875, Loss: 0.0368284210562706\n",
      "Batch 800/1875, Loss: 0.08458692580461502\n",
      "Batch 900/1875, Loss: 0.07170420140028\n",
      "Batch 1000/1875, Loss: 0.3991701602935791\n",
      "Batch 1100/1875, Loss: 0.24175263941287994\n",
      "Batch 1200/1875, Loss: 0.1134214699268341\n",
      "Batch 1300/1875, Loss: 0.0595841184258461\n",
      "Batch 1400/1875, Loss: 0.051264774054288864\n",
      "Batch 1500/1875, Loss: 0.048219047486782074\n",
      "Batch 1600/1875, Loss: 0.10665924847126007\n",
      "Batch 1700/1875, Loss: 0.02214799076318741\n",
      "Batch 1800/1875, Loss: 0.057762838900089264\n",
      "Epoch 3/5\n",
      "Batch 0/1875, Loss: 0.04231288284063339\n",
      "Batch 100/1875, Loss: 0.049724578857421875\n",
      "Batch 200/1875, Loss: 0.11658594757318497\n",
      "Batch 300/1875, Loss: 0.00962915550917387\n",
      "Batch 400/1875, Loss: 0.06783784925937653\n",
      "Batch 500/1875, Loss: 0.15357036888599396\n",
      "Batch 600/1875, Loss: 0.03746806085109711\n",
      "Batch 700/1875, Loss: 0.018281172960996628\n",
      "Batch 800/1875, Loss: 0.07892704010009766\n",
      "Batch 900/1875, Loss: 0.04469204321503639\n",
      "Batch 1000/1875, Loss: 0.23766343295574188\n",
      "Batch 1100/1875, Loss: 0.23363173007965088\n",
      "Batch 1200/1875, Loss: 0.08350755274295807\n",
      "Batch 1300/1875, Loss: 0.03642537072300911\n",
      "Batch 1400/1875, Loss: 0.02243388630449772\n",
      "Batch 1500/1875, Loss: 0.018566695973277092\n",
      "Batch 1600/1875, Loss: 0.0752139762043953\n",
      "Batch 1700/1875, Loss: 0.012845460325479507\n",
      "Batch 1800/1875, Loss: 0.02786780707538128\n",
      "Epoch 4/5\n",
      "Batch 0/1875, Loss: 0.021218176931142807\n",
      "Batch 100/1875, Loss: 0.049142807722091675\n",
      "Batch 200/1875, Loss: 0.05130039155483246\n",
      "Batch 300/1875, Loss: 0.0084768645465374\n",
      "Batch 400/1875, Loss: 0.05759979039430618\n",
      "Batch 500/1875, Loss: 0.09339042007923126\n",
      "Batch 600/1875, Loss: 0.027093172073364258\n",
      "Batch 700/1875, Loss: 0.006342257838696241\n",
      "Batch 800/1875, Loss: 0.04640883207321167\n",
      "Batch 900/1875, Loss: 0.029321908950805664\n",
      "Batch 1000/1875, Loss: 0.09858682751655579\n",
      "Batch 1100/1875, Loss: 0.1973492056131363\n",
      "Batch 1200/1875, Loss: 0.07201141119003296\n",
      "Batch 1300/1875, Loss: 0.02659127674996853\n",
      "Batch 1400/1875, Loss: 0.016168886795639992\n",
      "Batch 1500/1875, Loss: 0.009339461103081703\n",
      "Batch 1600/1875, Loss: 0.040698371827602386\n",
      "Batch 1700/1875, Loss: 0.00655449740588665\n",
      "Batch 1800/1875, Loss: nan\n",
      "Epoch 5/5\n",
      "Batch 0/1875, Loss: nan\n",
      "Batch 100/1875, Loss: nan\n",
      "Batch 200/1875, Loss: nan\n",
      "Batch 300/1875, Loss: nan\n",
      "Batch 400/1875, Loss: nan\n",
      "Batch 500/1875, Loss: nan\n",
      "Batch 600/1875, Loss: nan\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "# Load and preprocess data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize pixel values to [0, 1]\n",
    "x_train = x_train.reshape(-1, 28 * 28)  # Flatten the images\n",
    "x_test = x_test.reshape(-1, 28 * 28)\n",
    "y_train = to_categorical(y_train, 10)  # One-hot encode labels\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Create the model\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(28 * 28,)),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Define the RMSProp optimizer from scratch\n",
    "class RMSProp:\n",
    "    def __init__(self, learning_rate=0.001, beta=0.9, epsilon=1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta = beta  # Decay rate for the moving average of squared gradients\n",
    "        self.epsilon = epsilon  # Small constant to avoid division by zero\n",
    "        self.v = {}  # Dictionary to store the moving average of squared gradients\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        \"\"\"\n",
    "        Update parameters using the RMSProp optimizer.\n",
    "        :param params: Dictionary of model parameters (weights and biases).\n",
    "        :param grads: Dictionary of gradients corresponding to the parameters.\n",
    "        \"\"\"\n",
    "        for key in params:\n",
    "            if key not in self.v:\n",
    "                # Initialize the moving average of squared gradients for this parameter\n",
    "                self.v[key] = tf.zeros_like(params[key])\n",
    "\n",
    "            # Update the moving average of squared gradients\n",
    "            self.v[key] = self.beta * self.v[key] + (1 - self.beta) * tf.square(grads[key])\n",
    "\n",
    "            # Update the parameter using the RMSProp formula\n",
    "            params[key].assign_sub(self.learning_rate * grads[key] / (tf.sqrt(self.v[key]) + self.epsilon))\n",
    "\n",
    "# Loss function (categorical cross-entropy)\n",
    "def categorical_crossentropy(y_true, y_pred):\n",
    "    return -tf.reduce_mean(tf.reduce_sum(y_true * tf.math.log(y_pred), axis=1))\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, x_train, y_train, epochs=5, batch_size=32, learning_rate=0.001):\n",
    "    optimizer = RMSProp(learning_rate=learning_rate)\n",
    "    num_samples = x_train.shape[0]\n",
    "    num_batches = int(np.ceil(num_samples / batch_size))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        for batch in range(num_batches):\n",
    "            # Get a batch of data\n",
    "            start = batch * batch_size\n",
    "            end = start + batch_size\n",
    "            x_batch = x_train[start:end]\n",
    "            y_batch = y_train[start:end]\n",
    "\n",
    "            # Forward pass\n",
    "            with tf.GradientTape() as tape:\n",
    "                y_pred = model(x_batch, training=True)\n",
    "                loss = categorical_crossentropy(y_batch, y_pred)\n",
    "\n",
    "            # Backward pass (compute gradients)\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "            # Convert gradients and parameters to dictionaries for RMSProp\n",
    "            grads_dict = {f'w{i}': grad for i, grad in enumerate(grads)}\n",
    "            params_dict = {f'w{i}': param for i, param in enumerate(model.trainable_variables)}\n",
    "\n",
    "            # Update weights using the custom RMSProp optimizer\n",
    "            optimizer.update(params_dict, grads_dict)\n",
    "\n",
    "            # Print loss every 100 batches\n",
    "            if batch % 100 == 0:\n",
    "                print(f\"Batch {batch}/{num_batches}, Loss: {loss.numpy()}\")\n",
    "\n",
    "# Train the model\n",
    "train_model(model, x_train, y_train, epochs=5, batch_size=32, learning_rate=0.001)\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate_model(model, x_test, y_test):\n",
    "    y_pred = model(x_test)\n",
    "    y_pred = tf.argmax(y_pred, axis=1)\n",
    "    y_true = tf.argmax(y_test, axis=1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(y_pred, y_true), tf.float32))\n",
    "    print(f\"Test Accuracy: {accuracy.numpy()}\")\n",
    "\n",
    "evaluate_model(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04ace2e-26b1-4a69-89b4-3b156ceccf90",
   "metadata": {},
   "source": [
    "3. Root Mean Squared Propagation (RMSprop) from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc703164-d03d-44ad-8c62-016d5bf26865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Batch 0/1875, Loss: 2.474738121032715\n",
      "Batch 100/1875, Loss: 0.5045326948165894\n",
      "Batch 200/1875, Loss: 0.37019145488739014\n",
      "Batch 300/1875, Loss: 0.20342355966567993\n",
      "Batch 400/1875, Loss: 0.191922128200531\n",
      "Batch 500/1875, Loss: 0.38823366165161133\n",
      "Batch 600/1875, Loss: 0.16828876733779907\n",
      "Batch 700/1875, Loss: 0.13100148737430573\n",
      "Batch 800/1875, Loss: 0.159723699092865\n",
      "Batch 900/1875, Loss: 0.11896780878305435\n",
      "Batch 1000/1875, Loss: 0.46159279346466064\n",
      "Batch 1100/1875, Loss: 0.17954494059085846\n",
      "Batch 1200/1875, Loss: 0.17885930836200714\n",
      "Batch 1300/1875, Loss: 0.1602754294872284\n",
      "Batch 1400/1875, Loss: 0.2274518609046936\n",
      "Batch 1500/1875, Loss: 0.16448281705379486\n",
      "Batch 1600/1875, Loss: 0.19772203266620636\n",
      "Batch 1700/1875, Loss: 0.06007068604230881\n",
      "Batch 1800/1875, Loss: 0.16574259102344513\n",
      "Epoch 2/5\n",
      "Batch 0/1875, Loss: 0.08356095850467682\n",
      "Batch 100/1875, Loss: 0.1930152177810669\n",
      "Batch 200/1875, Loss: 0.16765405237674713\n",
      "Batch 300/1875, Loss: 0.048770565539598465\n",
      "Batch 400/1875, Loss: 0.12714283168315887\n",
      "Batch 500/1875, Loss: 0.18780554831027985\n",
      "Batch 600/1875, Loss: 0.06622470170259476\n",
      "Batch 700/1875, Loss: 0.060064613819122314\n",
      "Batch 800/1875, Loss: 0.06482170522212982\n",
      "Batch 900/1875, Loss: 0.05307283625006676\n",
      "Batch 1000/1875, Loss: 0.3627305030822754\n",
      "Batch 1100/1875, Loss: 0.11905541270971298\n",
      "Batch 1200/1875, Loss: 0.11386032402515411\n",
      "Batch 1300/1875, Loss: 0.1065753623843193\n",
      "Batch 1400/1875, Loss: 0.08748859167098999\n",
      "Batch 1500/1875, Loss: 0.12375715374946594\n",
      "Batch 1600/1875, Loss: 0.19069117307662964\n",
      "Batch 1700/1875, Loss: 0.018969010561704636\n",
      "Batch 1800/1875, Loss: 0.07290636748075485\n",
      "Epoch 3/5\n",
      "Batch 0/1875, Loss: 0.04444943740963936\n",
      "Batch 100/1875, Loss: 0.10280723124742508\n",
      "Batch 200/1875, Loss: 0.09408313781023026\n",
      "Batch 300/1875, Loss: 0.029170800000429153\n",
      "Batch 400/1875, Loss: 0.10638968646526337\n",
      "Batch 500/1875, Loss: 0.13017702102661133\n",
      "Batch 600/1875, Loss: 0.03721904754638672\n",
      "Batch 700/1875, Loss: 0.03348299860954285\n",
      "Batch 800/1875, Loss: 0.049005985260009766\n",
      "Batch 900/1875, Loss: 0.01178126223385334\n",
      "Batch 1000/1875, Loss: 0.19261766970157623\n",
      "Batch 1100/1875, Loss: 0.08199521899223328\n",
      "Batch 1200/1875, Loss: 0.1196257695555687\n",
      "Batch 1300/1875, Loss: 0.06407579779624939\n",
      "Batch 1400/1875, Loss: 0.035133615136146545\n",
      "Batch 1500/1875, Loss: 0.0784638375043869\n",
      "Batch 1600/1875, Loss: 0.15330708026885986\n",
      "Batch 1700/1875, Loss: 0.008484456688165665\n",
      "Batch 1800/1875, Loss: 0.034066688269376755\n",
      "Epoch 4/5\n",
      "Batch 0/1875, Loss: 0.031678035855293274\n",
      "Batch 100/1875, Loss: 0.06273643672466278\n",
      "Batch 200/1875, Loss: 0.10179965198040009\n",
      "Batch 300/1875, Loss: 0.014407731592655182\n",
      "Batch 400/1875, Loss: 0.07262593507766724\n",
      "Batch 500/1875, Loss: 0.10046648234128952\n",
      "Batch 600/1875, Loss: 0.04919049143791199\n",
      "Batch 700/1875, Loss: 0.03648752346634865\n",
      "Batch 800/1875, Loss: 0.038646481931209564\n",
      "Batch 900/1875, Loss: 0.0045511978678405285\n",
      "Batch 1000/1875, Loss: 0.1596176028251648\n",
      "Batch 1100/1875, Loss: 0.052337050437927246\n",
      "Batch 1200/1875, Loss: 0.09333059936761856\n",
      "Batch 1300/1875, Loss: 0.04825131595134735\n",
      "Batch 1400/1875, Loss: 0.021339528262615204\n",
      "Batch 1500/1875, Loss: 0.0293191559612751\n",
      "Batch 1600/1875, Loss: 0.13230903446674347\n",
      "Batch 1700/1875, Loss: 0.004966612905263901\n",
      "Batch 1800/1875, Loss: 0.02118760533630848\n",
      "Epoch 5/5\n",
      "Batch 0/1875, Loss: 0.03920302167534828\n",
      "Batch 100/1875, Loss: 0.04624222591519356\n",
      "Batch 200/1875, Loss: 0.11386719346046448\n",
      "Batch 300/1875, Loss: 0.00985301285982132\n",
      "Batch 400/1875, Loss: 0.052767325192689896\n",
      "Batch 500/1875, Loss: 0.07467114180326462\n",
      "Batch 600/1875, Loss: 0.05199739709496498\n",
      "Batch 700/1875, Loss: 0.048415977507829666\n",
      "Batch 800/1875, Loss: 0.03325142338871956\n",
      "Batch 900/1875, Loss: 0.0029975902289152145\n",
      "Batch 1000/1875, Loss: 0.1258714497089386\n",
      "Batch 1100/1875, Loss: 0.020386384800076485\n",
      "Batch 1200/1875, Loss: 0.05798310413956642\n",
      "Batch 1300/1875, Loss: 0.030495936051011086\n",
      "Batch 1400/1875, Loss: 0.01037103496491909\n",
      "Batch 1500/1875, Loss: 0.015415845438838005\n",
      "Batch 1600/1875, Loss: 0.09586546570062637\n",
      "Batch 1700/1875, Loss: 0.003958098124712706\n",
      "Batch 1800/1875, Loss: 0.02234264276921749\n",
      "Test Accuracy: 0.9714999794960022\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "# Load and preprocess data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize pixel values to [0, 1]\n",
    "x_train = x_train.reshape(-1, 28 * 28)  # Flatten the images\n",
    "x_test = x_test.reshape(-1, 28 * 28)\n",
    "y_train = to_categorical(y_train, 10)  # One-hot encode labels\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Create the model\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(28 * 28,)),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Define the Adam optimizer from scratch\n",
    "class CustomAdam:\n",
    "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1  # Exponential decay rate for the first moment estimates\n",
    "        self.beta2 = beta2  # Exponential decay rate for the second moment estimates\n",
    "        self.epsilon = epsilon  # Small constant to avoid division by zero\n",
    "        self.m = None  # First moment vector (mean)\n",
    "        self.v = None  # Second moment vector (uncentered variance)\n",
    "        self.t = 0  # Time step (iteration counter)\n",
    "\n",
    "    def update(self, weights, grads):\n",
    "        \"\"\"\n",
    "        Update weights using the Adam optimizer.\n",
    "        :param weights: List of model weights (trainable variables).\n",
    "        :param grads: List of gradients corresponding to the weights.\n",
    "        \"\"\"\n",
    "        if self.m is None:\n",
    "            # Initialize first and second moment vectors\n",
    "            self.m = [tf.zeros_like(w) for w in weights]\n",
    "            self.v = [tf.zeros_like(w) for w in weights]\n",
    "\n",
    "        self.t += 1  # Increment time step\n",
    "\n",
    "        for i in range(len(weights)):\n",
    "            # Update biased first moment estimate (m)\n",
    "            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grads[i]\n",
    "            # Update biased second moment estimate (v)\n",
    "            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * tf.square(grads[i])\n",
    "\n",
    "            # Compute bias-corrected first moment estimate\n",
    "            m_hat = self.m[i] / (1 - self.beta1 ** self.t)\n",
    "            # Compute bias-corrected second moment estimate\n",
    "            v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n",
    "\n",
    "            # Update weights\n",
    "            weights[i].assign_sub(self.learning_rate * m_hat / (tf.sqrt(v_hat) + self.epsilon))\n",
    "\n",
    "# Loss function (categorical cross-entropy)\n",
    "def categorical_crossentropy(y_true, y_pred):\n",
    "    return -tf.reduce_mean(tf.reduce_sum(y_true * tf.math.log(y_pred), axis=1))\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, x_train, y_train, epochs=5, batch_size=32, learning_rate=0.001):\n",
    "    optimizer = CustomAdam(learning_rate=learning_rate)\n",
    "    num_samples = x_train.shape[0]\n",
    "    num_batches = int(np.ceil(num_samples / batch_size))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        for batch in range(num_batches):\n",
    "            # Get a batch of data\n",
    "            start = batch * batch_size\n",
    "            end = start + batch_size\n",
    "            x_batch = x_train[start:end]\n",
    "            y_batch = y_train[start:end]\n",
    "\n",
    "            # Forward pass\n",
    "            with tf.GradientTape() as tape:\n",
    "                y_pred = model(x_batch, training=True)\n",
    "                loss = categorical_crossentropy(y_batch, y_pred)\n",
    "\n",
    "            # Backward pass (compute gradients)\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "            # Update weights using the custom Adam optimizer\n",
    "            optimizer.update(model.trainable_variables, grads)\n",
    "\n",
    "            # Print loss every 100 batches\n",
    "            if batch % 100 == 0:\n",
    "                print(f\"Batch {batch}/{num_batches}, Loss: {loss.numpy()}\")\n",
    "\n",
    "# Train the model\n",
    "train_model(model, x_train, y_train, epochs=5, batch_size=32, learning_rate=0.001)\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate_model(model, x_test, y_test):\n",
    "    y_pred = model(x_test)\n",
    "    y_pred = tf.argmax(y_pred, axis=1)\n",
    "    y_true = tf.argmax(y_test, axis=1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(y_pred, y_true), tf.float32))\n",
    "    print(f\"Test Accuracy: {accuracy.numpy()}\")\n",
    "\n",
    "evaluate_model(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be7473d-7ddf-4ff4-8b04-60c002293938",
   "metadata": {},
   "source": [
    "4. Stochastic Gradient Descent (SGD) using tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfc61dd9-3134-4c40-980a-43fbc7178fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7035 - loss: 1.1322 - val_accuracy: 0.9011 - val_loss: 0.3780\n",
      "Epoch 2/5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8924 - loss: 0.3839 - val_accuracy: 0.9150 - val_loss: 0.3076\n",
      "Epoch 3/5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9097 - loss: 0.3232 - val_accuracy: 0.9231 - val_loss: 0.2756\n",
      "Epoch 4/5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9201 - loss: 0.2821 - val_accuracy: 0.9296 - val_loss: 0.2535\n",
      "Epoch 5/5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9296 - loss: 0.2560 - val_accuracy: 0.9330 - val_loss: 0.2369\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9246 - loss: 0.2717\n",
      "Test accuracy: 0.9340000152587891\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.datasets import mnist \n",
    "from tensorflow.keras.utils import to_categorical # Load and preprocess data\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data() \n",
    "x_train, x_test =x_train / 255.0, x_test / 255.0 \n",
    "x_train = x_train.reshape(-1, 28 * 28)\n",
    "x_test = x_test.reshape(-1, 28 * 28) \n",
    "y_train = to_categorical(y_train, 10) \n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Create and compile the model \n",
    "model = Sequential([Dense(128, activation='relu', input_shape=(28 * 28,)), Dense(10, activation='softmax')])\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train and evaluate the model\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=0.2) \n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(\"Test accuracy:\",test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e90df0-23d1-4b4e-a02d-065b0c656c85",
   "metadata": {},
   "source": [
    "5. Adaptive Moment Estimation (Adam) using tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b09fead-20ab-4b8a-b6ee-ca6dd9c53bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.8626 - loss: 0.4759 - val_accuracy: 0.9572 - val_loss: 0.1536\n",
      "Epoch 2/5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9587 - loss: 0.1384 - val_accuracy: 0.9664 - val_loss: 0.1157\n",
      "Epoch 3/5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9734 - loss: 0.0896 - val_accuracy: 0.9691 - val_loss: 0.1018\n",
      "Epoch 4/5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9817 - loss: 0.0629 - val_accuracy: 0.9679 - val_loss: 0.1089\n",
      "Epoch 5/5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9850 - loss: 0.0483 - val_accuracy: 0.9747 - val_loss: 0.0852\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9738 - loss: 0.0837\n",
      "Test accuracy: 0.9771999716758728\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.datasets import mnist \n",
    "from tensorflow.keras.utils import to_categorical # Load and preprocess data\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data() \n",
    "x_train, x_test =x_train / 255.0, x_test / 255.0 \n",
    "x_train = x_train.reshape(-1, 28 * 28)\n",
    "x_test = x_test.reshape(-1, 28 * 28) \n",
    "y_train = to_categorical(y_train, 10) \n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Create and compile the model \n",
    "model = Sequential([Dense(128, activation='relu', input_shape=(28 * 28,)), Dense(10, activation='softmax')])\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train and evaluate the model\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=0.2) \n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(\"Test accuracy:\",test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f3515d-7676-45bf-b61b-128b0ff838cb",
   "metadata": {},
   "source": [
    "6. Root Mean Squared Propagation (RMSprop) using tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bca75ad2-ffb3-4f57-8bff-5ab0b8444d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8743 - loss: 0.4492 - val_accuracy: 0.9539 - val_loss: 0.1587\n",
      "Epoch 2/5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9591 - loss: 0.1387 - val_accuracy: 0.9663 - val_loss: 0.1212\n",
      "Epoch 3/5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9716 - loss: 0.0954 - val_accuracy: 0.9683 - val_loss: 0.1111\n",
      "Epoch 4/5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9788 - loss: 0.0722 - val_accuracy: 0.9735 - val_loss: 0.0948\n",
      "Epoch 5/5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9831 - loss: 0.0582 - val_accuracy: 0.9743 - val_loss: 0.0998\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9727 - loss: 0.0966\n",
      "Test accuracy: 0.9754999876022339\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.datasets import mnist \n",
    "from tensorflow.keras.utils import to_categorical # Load and preprocess data\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data() \n",
    "x_train, x_test =x_train / 255.0, x_test / 255.0 \n",
    "x_train = x_train.reshape(-1, 28 * 28)\n",
    "x_test = x_test.reshape(-1, 28 * 28) \n",
    "y_train = to_categorical(y_train, 10) \n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Create and compile the model \n",
    "model = Sequential([Dense(128, activation='relu', input_shape=(28 * 28,)), Dense(10, activation='softmax')])\n",
    "model.compile(optimizer='rmsprop' ,loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train and evaluate the model\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=0.2) \n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(\"Test accuracy:\",test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc544268-afa6-401e-bb40-83cbe1b31fb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
